{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG with Knowledge Graphs using Neo4j\n",
        "\n",
        "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using:\n",
        "- **Neo4j** as a knowledge graph database\n",
        "- **LangChain** for orchestration\n",
        "- **Groq API** (Llama 3.3 70B) for LLM capabilities\n",
        "- **Wikipedia** as data source\n",
        "- **HuggingFace embeddings** for vector search\n",
        "\n",
        "## Features\n",
        "- Graph-based knowledge representation\n",
        "- Hybrid search (vector + graph traversal)\n",
        "- Entity extraction and relationship mapping\n",
        "- Conversational interface with memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain==0.2.0 \\\n",
        "    langchain-community==0.2.1 \\\n",
        "    langchain-openai==0.1.7 \\\n",
        "    langchain-experimental==0.0.59 \\\n",
        "    neo4j==5.20 \\\n",
        "    wikipedia==1.4.0 \\\n",
        "    tiktoken==0.7.0 \\\n",
        "    sentence-transformers==2.7.0 \\\n",
        "    yfiles-jupyter-graphs-for-neo4j==1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "\n",
        "# LangChain core imports\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "\n",
        "# Visualization\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from neo4j import GraphDatabase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n",
        "\n",
        "**Important:** Create a `config.py` file with your credentials:\n",
        "```python\n",
        "GROQ_API_KEY = \"your-groq-api-key\"\n",
        "NEO4J_URI = \"neo4j+s://your-instance.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"your-password\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Import from config file (recommended)\n",
        "try:\n",
        "    from config import GROQ_API_KEY, NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD\n",
        "except ImportError:\n",
        "    # Option 2: Set directly (not recommended for production)\n",
        "    GROQ_API_KEY = \"your-groq-api-key\"\n",
        "    NEO4J_URI = \"neo4j+s://your-instance.databases.neo4j.io\"\n",
        "    NEO4J_USERNAME = \"neo4j\"\n",
        "    NEO4J_PASSWORD = \"your-password\"\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize LLM and Graph Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Groq LLM (using OpenAI-compatible endpoint)\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "# Test LLM connection\n",
        "print(\"Testing LLM connection...\")\n",
        "response = llm.invoke('Hello')\n",
        "print(f\"LLM Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Neo4j graph\n",
        "print(\"Connecting to Neo4j...\")\n",
        "graph = Neo4jGraph()\n",
        "print(\"Successfully connected to Neo4j!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Wikipedia data\n",
        "print(\"Loading Wikipedia data...\")\n",
        "raw_documents = WikipediaLoader(query=\"Elizabeth I\").load()\n",
        "print(f\"Loaded {len(raw_documents)} documents\")\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
        "documents = text_splitter.split_documents(raw_documents[:3])\n",
        "print(f\"Split into {len(documents)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build Knowledge Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert documents to graph\n",
        "print(\"Converting documents to knowledge graph...\")\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "print(f\"Generated {len(graph_documents)} graph documents\")\n",
        "\n",
        "# Add to Neo4j\n",
        "print(\"Adding graph documents to Neo4j...\")\n",
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")\n",
        "print(\"Knowledge graph created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Setup Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize embeddings\n",
        "print(\"Loading embedding model...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Create vector index\n",
        "print(\"Creating vector index...\")\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    embeddings,\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")\n",
        "print(\"Vector index created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create full-text index for entity search\n",
        "graph.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\"\n",
        ")\n",
        "print(\"Full-text index created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Entity Extraction Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define entity schema\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the person, organization, or business entities that appear in the text\",\n",
        "    )\n",
        "\n",
        "# Create entity extraction chain\n",
        "entity_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are extracting organization and person entities from the text.\"),\n",
        "    (\"human\", \"Use the given format to extract information from the following input: {question}\"),\n",
        "])\n",
        "\n",
        "entity_chain = entity_prompt | llm.with_structured_output(Entities)\n",
        "\n",
        "# Test entity extraction\n",
        "test_entities = entity_chain.invoke({\"question\": \"Who is Elizabeth I?\"})\n",
        "print(f\"Extracted entities: {test_entities.names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Structured Retrieval Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"Generate fuzzy full-text search query for Neo4j.\"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "\n",
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"Retrieve structured data from knowledge graph based on entities.\"\"\"\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    \n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def retriever(question: str) -> str:\n",
        "    \"\"\"Hybrid retrieval combining structured graph data and vector search.\"\"\"\n",
        "    print(f\"Search query: {question}\")\n",
        "    \n",
        "    # Get structured data from graph\n",
        "    structured_data = structured_retriever(question)\n",
        "    \n",
        "    # Get unstructured data from vector search\n",
        "    unstructured_data = [\n",
        "        el.page_content for el in vector_index.similarity_search(question)\n",
        "    ]\n",
        "    \n",
        "    # Combine both data sources\n",
        "    final_data = f\"\"\"Structured data:\n",
        "{structured_data}\n",
        "\n",
        "Unstructured data:\n",
        "{\"#Document \".join(unstructured_data)}\n",
        "    \"\"\"\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test structured retrieval\n",
        "test_result = structured_retriever(\"Who is Elizabeth I?\")\n",
        "print(\"Sample structured data:\")\n",
        "print(test_result[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Conversational RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat history formatting\n",
        "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
        "    \"\"\"Format chat history into message objects.\"\"\"\n",
        "    buffer = []\n",
        "    for human, ai in chat_history:\n",
        "        buffer.append(HumanMessage(content=human))\n",
        "        buffer.append(AIMessage(content=ai))\n",
        "    return buffer\n",
        "\n",
        "\n",
        "# Question condensation for follow-up queries\n",
        "condense_template = \"\"\"Given the following conversation and a follow up question, \n",
        "rephrase the follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Follow Up Input: {question}\n",
        "\n",
        "Standalone question:\"\"\"\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)\n",
        "\n",
        "\n",
        "_search_query = RunnableBranch(\n",
        "    # If input includes chat_history, condense it with the follow-up question\n",
        "    (\n",
        "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
        "            run_name=\"HasChatHistoryCheck\"\n",
        "        ),\n",
        "        RunnablePassthrough.assign(\n",
        "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "        )\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | llm\n",
        "        | StrOutputParser(),\n",
        "    ),\n",
        "    # Else, just pass through the question\n",
        "    RunnableLambda(lambda x: x[\"question\"]),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Answer generation prompt\n",
        "answer_template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Use natural language and be concise.\n",
        "Answer:\"\"\"\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_template(answer_template)\n",
        "\n",
        "# Build the complete RAG chain\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": _search_query | retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | answer_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Query Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Simple query\n",
        "question1 = \"Which house did Elizabeth I belong to?\"\n",
        "answer1 = chain.invoke({\"question\": question1})\n",
        "print(f\"Q: {question1}\")\n",
        "print(f\"A: {answer1}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Follow-up query with context\n",
        "question2 = \"When was she born?\"\n",
        "chat_history = [(question1, answer1)]\n",
        "answer2 = chain.invoke({\n",
        "    \"question\": question2,\n",
        "    \"chat_history\": chat_history\n",
        "})\n",
        "print(f\"Q: {question2}\")\n",
        "print(f\"A: {answer2}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Complex relationship query\n",
        "question3 = \"Who were Elizabeth I's parents and siblings?\"\n",
        "answer3 = chain.invoke({\"question\": question3})\n",
        "print(f\"Q: {question3}\")\n",
        "print(f\"A: {answer3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Graph Visualization (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable custom widgets in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "def show_graph(cypher: str = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"):\n",
        "    \"\"\"Visualize Neo4j graph using yFiles.\"\"\"\n",
        "    driver = GraphDatabase.driver(\n",
        "        uri=os.environ[\"NEO4J_URI\"],\n",
        "        auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
        "    )\n",
        "    session = driver.session()\n",
        "    widget = GraphWidget(graph=session.run(cypher).graph())\n",
        "    widget.node_label_mapping = 'id'\n",
        "    display(widget)\n",
        "    return widget\n",
        "\n",
        "\n",
        "# Uncomment to visualize the graph\n",
        "# show_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Interactive Query Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_interface():\n",
        "    \"\"\"Simple chat interface with memory.\"\"\"\n",
        "    chat_history = []\n",
        "    print(\"RAG Knowledge Graph Chatbot (type 'quit' to exit)\\n\")\n",
        "    \n",
        "    while True:\n",
        "        question = input(\"You: \").strip()\n",
        "        \n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        \n",
        "        if not question:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            # Get answer with chat history\n",
        "            answer = chain.invoke({\n",
        "                \"question\": question,\n",
        "                \"chat_history\": chat_history\n",
        "            })\n",
        "            \n",
        "            print(f\"\\nBot: {answer}\\n\")\n",
        "            \n",
        "            # Update chat history\n",
        "            chat_history.append((question, answer))\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\\n\")\n",
        "\n",
        "\n",
        "# Uncomment to start interactive chat\n",
        "# chat_interface()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete RAG pipeline with:\n",
        "- ✅ Knowledge graph construction from Wikipedia\n",
        "- ✅ Hybrid retrieval (graph + vector search)\n",
        "- ✅ Entity extraction and relationship mapping\n",
        "- ✅ Conversational interface with memory\n",
        "- ✅ Integration with Groq's Llama 3.3 70B model\n",
        "\n",
        "### Next Steps\n",
        "1. Expand the knowledge base with more documents\n",
        "2. Fine-tune entity extraction for your domain\n",
        "3. Add custom relationship types\n",
        "4. Implement caching for better performance\n",
        "5. Add evaluation metrics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
